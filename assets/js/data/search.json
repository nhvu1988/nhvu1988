[ { "title": "How to use Cloudflare Access to protect Azure Web App", "url": "/posts/how-to-use-cloudflare-access-to-protect-azure-web-app/", "categories": "DevOps", "tags": "azure, cloudflare", "date": "2022-01-28 18:00:00 +0700", "snippet": "While deploying my application to Azure Web App, I would like to use the Cloudflare Access to protect my web application. So that only I can use the web application.But the problem is the Azure Web App always assigns a custom domain to the web app (ex webapp-name.azurewebservices.net) so it could be scanned and accessed in public if anyone knows that domain. How we can prevent that?It could be resolved if we can restrict access to the app from everywhere but Cloudflare Access.There‚Äôre 4 steps we need to be done. Configure the Cloudflare Access to deny all. Add custom domain for Azure Web App. Enable the proxy mode for the DNS on cloudflare. Restrict all the traffic to the Azure Web App to be allowed from Cloudflare only.1. Configure the Cloudflare Access to deny userGo to Cloudflare Access and add an application with a policy to deny all users (except mine :D)2. Add custom domain for Azure Web AppEnsure that the custom domain has been added and configured on Azure Web App‚Ä¶3. Enable proxy mode for Cloudflare DNS‚Ä¶and Cloudflare as well, but remember to enable proxy mode for it. So now all the traffic to Azure Web App will be proxied to Cloudflare.4. Restrict all the traffic to the Azure Web AppGo to Networking - Access restrictionAdd the Cloudflare Outbound IP Ranges to the rules.But the issue is Azure Portal does not allow to add more than one IP range so you will see this error when adding it.So you need to run the powershell scripts to add those IP Ranges directly on the Azure Portal Cloud Shell. Click on the Cloud Shell icon on the top-right of the portal, change the command prompt to PowerShellAnd run the following commandsAdd-AzWebAppAccessRestrictionRule -ResourceGroupName &quot;resource-group-name&quot; -WebAppName &quot;web-app-name&quot; -Name &quot;Cloudflare Part 1&quot; -IpAddress &quot;173.245.48.0/20,103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,141.101.64.0/18,108.162.192.0/18,190.93.240.0/20,188.114.96.0/20&quot; -Priority 100 -Action AllowAdd-AzWebAppAccessRestrictionRule -ResourceGroupName &quot;resource-group-name&quot; -WebAppName &quot;web-app-name&quot; -Name &quot;Cloudflare Part 2&quot; -IpAddress &quot;197.234.240.0/22,198.41.128.0/17,162.158.0.0/15,104.16.0.0/13,104.24.0.0/14,172.64.0.0/13,131.0.72.0/22&quot; -Priority 100 -Action AllowWe need to slit them to 2 rules because of this error.Add-AzWebAppAccessRestrictionRule: Cannot validate argument on parameter &#39;IpAddress&#39;. Only 8 ip addresses are allowed per rule" }, { "title": "Azure DevOps - What I Have Done - Part 2", "url": "/posts/azure-devops-what-i-have-done-part-2/", "categories": "DevOps", "tags": "azure", "date": "2022-01-22 15:00:00 +0700", "snippet": "Continue to a (very long) previous post, I will continue to list out some questions but‚Ä¶ The answers will be in every single next post (or not ü§¶‚Äç‚ôÇÔ∏è) (again)Azure Portal How to use cloudflare access to protect the azure app service? -&amp;gt; PUBLISHED How to deploy a docker-based web app to azure app service using azure files as volume? How to add custom domain for app service and serve it with HTTPS without SSL installation or using lets enscrypt? How to deploy code-server to azure app service and start coding on browser? How to configure a created Web App for Containers to use the new image?" }, { "title": "Use OpenSSH in Windows", "url": "/posts/use-openssh-in-windows/", "categories": "DevOps", "tags": "openssh, windows", "date": "2021-03-14 07:00:00 +0700", "snippet": "Sometimes I manage some stuffs using OpenSSH such as convert certificate file from .key format to .pfx format. But it‚Äôs hard to find an official binary or installer (at least for me whenever I find it üòÇ). There‚Äôs a simple way to use OpenSSH on Windows if (already?) having git installed on the machine.Go to the C:\\Program Files\\Git\\usr\\bin\\ folder and run cmd from there. You can also run from git bash as well.There‚Äôre a lot of developer tools here including openssl.exe.An example command to convert PEM certificate to PFX certificate.openssl pkcs12 -export -out &quot;C:\\Users\\nhvu1988\\Desktop\\cert.pfx&quot; -inkey &quot;C:\\Users\\nhvu1988\\Desktop\\cert.key&quot; -in &quot;C:\\Users\\nhvu1988\\Desktop\\cert.pem&quot;That‚Äôs easy right? üòé Happy coding everyone!" }, { "title": "How to publish local web project to the internet", "url": "/posts/how-to-publish-local-web-project-to-the-internet/", "categories": "DevOps", "tags": "docker, cloudflared", "date": "2021-03-06 07:00:00 +0700", "snippet": "While developing project locally, sometimes I would like to publish them for client testing or having quickly demonstration. The local project is running on port 8080 http://localhost:8080Thanks to cloudflared that we can use to publish it to the internet easily with a simple single command.Using binary cloudflaredWe have to download the binary cloudflared and execute this commandcloudflared tunnel --no-autoupdate --url http://localhost:1337Using docker image cloudflare/cloudflaredIn case I want to publish a web project running on a container names mycontainerCheck which‚Äôs network &amp;amp; IP address mycontainer is running ondocker container inspect mycontainerRun the following command to publish itdocker run --rm --network=network_name cloudflare/cloudflared tunnel --no-autoupdate --url http://container_ip:8080" }, { "title": "Some useful docker commands", "url": "/posts/some-useful-docker-commands/", "categories": "DevOps", "tags": "docker", "date": "2020-07-26 07:00:00 +0700", "snippet": "Since docker is ready installed on the machine, when I want to run/build something, I just try to find a docker image and use it right away. So I collect some useful commands for my works, hope it‚Äôs also useful for yours.The docker run command will create and run the container but I just want to run it once and remove it after run so I put the option --rm to the command. The container will be removed when it exits or when the daemon exits, whichever happens first.The -it option is the combination of -i &amp;amp; -t for interactive process, so the command should have a same below pattern.docker run -it --rm image:tag [command]Node To run a node command, we can use node image with the command after that. We have to mount the current folder (using environment variable $PWD) to the working dir as well as define the working dir that node will start on. For example to run a single Node.js scriptdocker run -it --rm -v &quot;$PWD&quot;:/app -w /app node node single-script.js $PWD is a linux environment variable defines current folder that the command is running on. To specify running node version is 8.docker run -it --rm -v &quot;$PWD&quot;:/app -w /app node:8 node single-script.js Or to run npm installdocker run -it --rm -v &quot;$PWD&quot;:/app -w /app node npm installAngular CLI To run a ng command, we can use trion/ng-cli image. Since the image automatically sets the working dir to /app so we don‚Äôt need to define it but mounting the volumn as project folder to /app. For example to run the create new angular application.docker run -it --rm -v &quot;$PWD&quot;:/app trion/ng-cli ng new website And to generate a componentdocker run -it --rm -v &quot;$PWD&quot;:/app trion/ng-cli ng g c new-componentMongoDB To run a mongo command, we can use mongo image.Mongo 3.6 To generate a database backup with mongodumpdocker run -it --rm -v $PWD:/backup mongo:3.6 sh -c &quot;mongodump --uri=mongodb://username:password@mongo_uri:27017/dbname --authenticationDatabase=admin --archive=&#39;/backup/dbname.archive&#39;&quot;Mongo 4.2 To generate a database backup with mongodumpdocker run -it --rm -v $PWD:/backup mongo sh -c &quot;mongodump --uri=mongodb://username:password@mongo_uri:27017/dbname --authenticationDatabase=admin --archive=&#39;/backup/dbname.archive&#39;&quot; If the mongo server instance is running on localhost using docker-compose, get it‚Äôs network name with commanddocker network ls And use the network name in --net parameterdocker run -it --rm -v $PWD:/backup --net network_name mongo sh -c &quot;mongodump --uri=mongodb://username:password@mongo_container_name:27017/dbname --authenticationDatabase=admin --archive=&#39;/backup/dbname.archive&#39;&quot; To restore that database backup filedocker run -it --rm -v $PWD:/backup mongo sh -c &quot;mongorestore --uri=mongodb://username:password@mongouri:27017/dbname --authenticationDatabase=admin --archive=&#39;/backup/dbname.archive&#39;&quot;" }, { "title": "How I install git on Windows", "url": "/posts/how-i-install-git-on-windows/", "categories": "DevOps", "tags": "git, windows", "date": "2020-07-26 07:00:00 +0700", "snippet": "When installing git on Windows, we need to do some steps for installation and setup before working with it so I will keep track them below as well as the reason why I do that. If you have any suggestion to improve it, I‚Äôm very appreciated to know and learn to make it better.Since I will clone git using SSH as the article How to use git with SSH key so my setup will make sure it works with SSH as well.Download and install Git client Go to Git-scm and download the latest 64-bit version. Start the setup by double click the installer. Click next on first screen and in the Select component screen. I uncheck to Windows Explorer integration and Associate .sh files... because I don‚Äôt use those features and put more things to my mouse-right-click menu. Click next until the Choosing the default editor used by Git screen. I change it to Use Notepad++ as Git&#39;s default editor instead, since I would like to use Notepad++ because it‚Äôs light and fit to this job. Click next and go to Adjust your PATH environment screen and keep the recommended option because as descibed, it makes Git works well with any tools. Click next and in the Configuring the line ending conversions screen, I choose the Checkout as-is, commit as-is since I don‚Äôt want Git to do any conversion when doing checkout/commit. This option could be overrided later with git command $ git config --global core.autocrlf false Click next and in the Configuring the terminal emulator to use with Git Bash, I choose to Use Windows&#39; default console window because as I said before, I don‚Äôt want to install any other terminal. Actually, I use other tools (PHPStorm and Windows Terminal) for this job so just keep thing light üòâ Click next and in the Choose the default behavior of git pull, I leave it as default (fast-forward or merge) Click next and in the Configuring extra options, I uncheck the Enable Git Credential Manager because I use SSH so no need to use this. But we should keep it if want to HTTPS and enable multi-factor authentication. Click next and install button in the last screen. I don‚Äôt want to try any experience feature because it‚Äôs working environment and should be stable as much as possible.Setup default email and user nameBefore doing any git commit, we need to setup git username and email with the commands below$ git config --global user.name &quot;Vu Nguyen&quot;$ git config --global user.email nhvu1988@gmail.comIt‚Äôs possible to config the specific email in a single repository. In the repository folder, run the following git config command$ git config user.email nhvu1988@working.mailSetup SSH keyThis step could be simple or complicated due to what we need, such as my requirement is Use only one SSH key for all machines.So I don‚Äôt have to manage the SSH key by creating a new one for every machine, and remember to remove it when not used anymore.Prepare SSH key Create an SSH key following this guide from github. Upload SSH key to git repositories following my guide.Configure SSH key Create a config file inside ~/.ssh/ folder. The folder is C:\\Users\\nhvu\\.ssh\\ on my machine but different username on yours. The file has no extension, remember removing .txt or anything else if creating using Windows Explorer. Add the content below and update ~/.ssh/id_rsa to your SSH key file path or keep it if you just create a new one. host * IdentityFile ~/.ssh/id_rsa (Optional) This step is not for me but for someone want to have specific SSH key for each host, you can add more record to the config file. host github.com IdentityFile ~/.ssh/id_rsa.github host gitlab.com IdentityFile ~/.ssh/id_rsa.gitlab " }, { "title": "What applications I will install after installing Windows?", "url": "/posts/what-applications-i-will-install-after-installing-windows/", "categories": "DevOps", "tags": "windows", "date": "2020-04-23 07:00:00 +0700", "snippet": "Today, I have to move everything in old laptop to the new one so I will try to list out all applications I need to install and how I can keep/reuse their configuration as well.Since I‚Äôm a developer so most of applications will be for development purpose.Installed applicationsBelow is the list of installed applications in the current laptop I will move out.Personal tools 7-Zip Adobe Acrobat Reader DC Bitwarden Calibre Chrome Microsoft Edge CCleaner Double Commander HEIF Image Extensions (installed from MS Store) HEVC Video Extensions from Device Manufacturer (installed from MS Store) IDM (Internet Download Manager) Logitech Options (required for my mouse M590) paint.net ShareX Skype (Desktop version) Startup Delayer Spotify Trello VLC media player Viber ZaloDeveloper tools Fork Git for Windows Icons8 Pichon Office 365 / Outlook / OneNote Postman Microsoft Azure Storage Explorer MSSMS (SQL Server Management Studio) Visio VS Code VS 2019 Notepad++ NVM for Windows Remote Desktop Connection Manager WinSCP Webp Image Extensions (installed from MS Store) Terminal PhpStorm FortiClient VPN Slack MSTeamsOthers ClumsyLeft TableXplorer ClumsyLeft CloudXplorer" }, { "title": "Create Windows Setup USB boot in BIOS and UEFI mode", "url": "/posts/create-windows-setup-usb-boot-in-bios-and-uefi-mode/", "categories": "DevOps", "tags": "bios, uefi", "date": "2020-04-20 07:00:00 +0700", "snippet": "As we know most new PC/laptops are using UEFI mode when booting into Windows Setup but I would like to support the legacy BIOS as well because sometimes I will support to help my mom and others are using the old PCs. So I will create a Windows Setup USB supports to run on any both of them using Rufus Download Windows ISO from Microsoft. Download Rufus. Run Rufus and in the Partition scheme, select MBR. Make sure the Target system change to BIOS (or UEFI-CSM). Click START to create the USB boot üòä" }, { "title": "How to fix error HRESULT = &#39;8000000A&#39; when building VS Installer", "url": "/posts/how-to-fix-error-8000000A-when-build-vs-installer/", "categories": "Coding", "tags": "vs installer", "date": "2020-01-03 00:00:00 +0700", "snippet": "It‚Äôs already answered here stackoverflow but I want to take note for later usage in case I forget it again ü§£. Unfortunately we couldn‚Äôt address all cases of the command line issue for this release as we‚Äôre still investigating the appropriate way to address them. What we do have is a workaround that we believe will work for almost all of them. If you are still suffering this issue then you can try to change the DWORD value for the following registry value EnableOutOfProcBuild to 0: VS2013 HKEY_CURRENT_USER\\Software\\Microsoft\\VisualStudio\\12.0_Config\\MSBuild VS2015 HKEY_CURRENT_USER\\Software\\Microsoft\\VisualStudio\\14.0_Config\\MSBuild If this doesn‚Äôt exist you can create it as a DWORD. kristian mo" }, { "title": "How to create .msi installer using VS Installer", "url": "/posts/how-to-create-msi-installer-using-vs-installer/", "categories": "Coding", "tags": "vs installer", "date": "2019-12-09 00:00:00 +0700", "snippet": "The VS installer extension is very old, out-dated and obsoleted by Microsoft but it‚Äôs still the quick and simple way to create an installer for any project. Since I just finished the task relates to it and have to fixed few (or a lot‚Ä¶ üòÇ) issues so this guide is the notes for them.Install Visual Studio Installer extensionOpen Visual Studio -&amp;gt; Tools -&amp;gt; Extensions and Updates -&amp;gt; Online and search for installerCreate Installer project for Windows applicationOpen Visual Studio -&amp;gt; File -&amp;gt; New Project -&amp;gt; Other Project Types -&amp;gt; Visual Studio Installer -&amp;gt; Setup WizardClick Next and select Project type suits to your application type, mine is Windows application so I choose Create a setup for a Windows ApplicationClick Next and in the step Choose files to include, we can add any optional files (readme, changelogs‚Ä¶) here. After that, click Finish to create the projectAdd application output to InstallerIn the Installer project, right click to project name -&amp;gt; View -&amp;gt; File SystemIn the File System on Target Machine -&amp;gt; Application Folder -&amp;gt; Add -&amp;gt; Project Output and select application project, it will automatically add all files in bin folder of your project build to the installer and it will copy them to the application folder while installing.Create custom dialog to allow user edit configuration variablesThe example is we would like to let user update the connection string of a SQL server database, it requires user to fill a database connection string (we assume that user know how to get it ‚úå)Since it‚Äôs a complex process so I separate it to few small steps belowCreate custom dialog to get input values from userRight click to Installer Project -&amp;gt; View -&amp;gt; User InterfaceIn the Install section, right click to Start -&amp;gt; Add DialogIn the Add Dialog popup, choose a suitable dialog for your requirement, I will pick the Textboxes (A) since we need to get input string only The Textboxes (A) will appear at the end of dialogs list, we have to move it to before Installation Folder dialog (and after Welcome dialog). Otherwise we will get an error during build process.Customize the custom dialogRight click to the Textboxes (A) and select Properties Windows to open Properties windows.In Properties windows, we disable all textboxes except the first one (that use for user inputing the connection string) by set the Edit2Visible, Edit3Visible and Edit4Visible to False.We change the Edit1Label to Database ConnectionString to make sure user know what‚Äôs value to input.You can change the value of Edit1Property property to anything but I just let it as default (EDITA1) and remember to use it later.This is the final custom dialog in installation process.Create new custom action projectSince the Visual Studio Installer only helps with the custom dialogs or custom actions during installation. We have to create a custom action (library) project to make a simple code to get the user input and update the App.config.In Visual Studio -&amp;gt; File -&amp;gt; New Project -&amp;gt; Class Library (.NET Framework)Add new Installer Class itemIn the new CustomInstaller class, we add the override Install function likes thisAdd the custom action project to Installer projectRight click to installer Project Name -&amp;gt; View -&amp;gt; Custom Actions -&amp;gt; Install -&amp;gt; Add Custom Action‚Ä¶Select Application Folder -&amp;gt; Add Output‚Ä¶ -&amp;gt; Primary output from custom action project nameNow, the code in Install function will be run during install process. If you want to do the custom action during uninstall process, do the same step 3 and 4 but change to override the Uninstall function and add the primary output to Uninstall section.Update custom action data with user input and default installation dataRight click to the Primary output from custom action project name we just added to Custom Actions in step 4 and select Properties Windows to open Properties windowsIn the Properties windows, fill the CustomActionData with values as below The [TARGETDIR]\\ is a default parameter of VS Installer, it will pass the application directory that user selects in Installation Folder dialog during install process. The [EDITA1] is the value of the Edit1Property input in custom dialog created in step 2. The keys targetdir and connectionstring will be passed to Context.Parameters so we can retrieve them in codes in next step.Retrieve user input in codeUpdate the code in Install function with below to get the custom dataNow we can update the connection string to app.config with the user filled value during install process.The sample codes could be found here Console.App.Installer." }, { "title": "A Simple ForeachAsync", "url": "/posts/a-simple-foreachasync/", "categories": "Coding", "tags": "parallel, async, dotnet", "date": "2018-12-17 00:00:00 +0700", "snippet": "It‚Äôs just the codes from Implementing a simple ForEachAsync, part 2 but I will write it as simple as it should beCodepublic static class ParallelHelper{ public static Task ForEachAsync&amp;lt;T&amp;gt;(this IEnumerable&amp;lt;T&amp;gt; source, int maxConcurrent, Func&amp;lt;T, Task&amp;gt; body) { return Task.WhenAll( from partition in Partitioner.Create(source).GetPartitions(maxConcurrent) select Task.Run(async delegate { using (partition) while (partition.MoveNext()) await body(partition.Current); })); }}How to usestring[] urls = new []{ &quot;https://google.com&quot;, &quot;https://bing.com&quot;, &quot;https://yahoo.com&quot;,};await ParallelHelper.ForEachAsync(urls, 5, async (url) =&amp;gt;{ WebClient client = new WebClient(); await client.DownloadStringTaskAsync(url);});" }, { "title": "Azure Develop-Build-Release Life Cycle", "url": "/posts/azure-develop-build-release-life-cycle/", "categories": "DevOps", "tags": "azure", "date": "2018-12-05 00:00:00 +0700", "snippet": "Azure provides many tools and services that fully and completely convenience for the software development, from code repository (source control), build, release and hosting application‚Ä¶ So in this post, I will introduce some concepts and fundamentals in my development process to work on Azure environment.Azure has two main places that we work on. The first one is Visual Studio Team Services (VSTS) - now called Azure DevOps Services - which mostly used by developers and operations teams to develop and deploy the application. The second is Azure Portal where we use to view and manage the application resources such as web app, database, virtual machine‚Ä¶ and it also provides strong monitor tools to trace down issues and maintenance of the application.In my sample website application, we have 3 projects, one is ASP.NET Web application, the second is NUnit Test project and the last is Azure Functions, all of them are created by Visual Studio in a single solution. As I would like to deploy them to two applications, one is an app service and another is Azure Functions in Azure environment.DevelopWe can use Git or TFS in VSTS to manage our codes and easily cooperate with any other Azure services, I won‚Äôt talk much this since Microsoft just bought Github so we can use Github as well. Since we use git-flow when developing so we always have 2 main branches, develop using for development only and master using for testing and deploying to production.Build PipelineThe main purpose of build tasks is compiling and verifying the source code, includes testing by performing unittest or integration test in a middle step as well. In the final step, it ends up with an artifact published that is ready to use in the release pipeline.An artifact is the outputs of build tasks, it mays contains the package of files that we want to produce or anything else we want to deploy to our application. In .NET project, it often contains a bin folder with some draft *.config files such as ApplicationInsights.config, web.config or app.config‚Ä¶A build pipeline could be used to build for a single project and publish one artifact or to build the whole solution with multiple projects that come with many artifacts in the final result. But I recommend that we should devide the solution to many single artifacts that will be run in many separated pipelines.Since in my example, I just want to deploy 2 applications so I will create 2 build pipelines for them. The first build is for compiling Web application, doing unit-tests in Test project and publish a website package artifact. The second one is for compiling and publishing Azure Functions artifact only. Totally, we have 4 build pipelines like thisAgent poolWhen creating a new build pipeline, we need to select an agent pool to run the build tasks. The build agent mays depend on which‚Äôs kind of your project and the VS2017 should work at all excepting your project need to be built in macOS, please change to Host macOS instead.In my example, we will select Hosted VS2017 to run, it‚Äôs just a hosted machine having built-in VS2017 including all necessary components for the development.Branch triggerWhenever code is pushed to main branches, we will trigger a build pipeline automatically. Firstly, we need to setup for the trigger when code is published to the main branch.In the Trigger tab of build pipeline, check to Enable continuous integration and select a main branch in Branch filters list.Build TasksA .NET build pipeline often contains 4 basic tasks.They should be automatically generated when we select a build template, or we can start with an empty task and add them manually later.Nuget RestoreThe first step is nuget restore, it needs to restore Nuget libraries before making build .NET application. If we want to build whole solution, add path to .sln file to restore, otherwise select packages.config in a project to build for a single project.Visual Studio BuildThe VSBuild task is very simple, just select the .sln to build whole solution or .csproj to build a single project. Other configurations could be keeped as default.The important config in this step is MSBuild Arguments where we can put some build arguments but we will discuss it in another post since every argument fits a specific build scenario.Visual Studio TestWe can use the default settings of the Visual Studio Test task and it will automatically detect and run test for all dlls having tests keyword, or we can have a simple modification in Test Assembly setting for compatible with our test project name. This task will run as the same with Test runner on Visual Studio same version.Publish Build ArtifactsWe just leave this task as default since it will copy all ouputs from artifact staging directory to a publish location in TFS server.Release PipelineAfter an artifact is ready, we will create new release from a release definition and start deploying to Azure resource. Each release could contains one environment (formally Stage in VSTS) like Dev or multiple environments such as Test, Demo, Prod in my example.Variables TransformBefore deploying to Azure, we will do an important step that is transform all draft variables to correct variables matching to every specific environment. For example, we have a variable Environment that our application will read and know which‚Äôs environment it‚Äôs running on. That variable stays in Web.config, its value is Local as I‚Äôm running locally but I would like to transform it to Dev, Test, Demo, or Prod when existing on the Azure app service.Pipeline VariablesWe can set that variable in the Pipeline variables and it will be automatically transformed during deployment process, just simply enable XML variable substitution in Azure App Service Deploy task.The pipeline variables will available to all tasks in the current release so it should contains all needed variables. It could be a appSettings in web.config or app.config file, the AppServiceName that we use in the deploy task, or anything else should be a dynamic variable.Variable GroupsThe variable groups are just the same and be used as the same with pipeline variables, excepts its values will available in cross releases. It means, all dynamic variables you want to use in multiple releases, just define and put them to a variable group in the Library sectionSo in my example, I will move the Environment variable to the variable group Dev since I would like that variable is available and can be read in Deploy AzureFunctions to Dev release as well. So I will link the variable group Dev to both Deploy Web to Dev and Deploy AzureFunctions to Dev.Now, all tasks in both releases to Dev can read the Environment with the value is Dev as the same.Continuous Deployment TriggerWe can enable the trigger on whenever an artifact is ready, a new release will be created and deploy application automatically.And for auto deploying to specific environment. It‚Äôs recommended to enable this trigger for development only, and other environments should be deployed in person.Azure PortalSince Azure Devops is used for development and deployment process, the Azure portal is the place for resource management and application monitoring. We will work with the application resource and its third party services, all available Azure services could be managed and accessed from there.Azure Portal provides a strong management and monitoring tool names Monitor combining from two services are Log Analytics and Application Insights. Both of them could be integrated and configurated easily in our application. They will automatically collect activity logs, diagnostic logs and telemetry from our applications and visualize it to portal, so we can check our application health, measure application performance‚Ä¶ARM templateARM stands for Azure Resource Manager, it‚Äôs a JSON file that contains all Azure resource‚Äôs information that we want to manage. So in my example, I will create a JSON that contains 2 resource items, the first one is an app service for the web application, the second is a Azure Functions for the functions app.{ &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;parameters&quot;: {}, &quot;variables&quot;: {}, &quot;resources&quot;: [ { &quot;name&quot;: &quot;Web Application Sample&quot;, &quot;type&quot;: &quot;Microsoft.Web/sites&quot;, &quot;apiVersion&quot;: &quot;2016-08-01&quot;, &quot;location&quot;: &quot;Southeast Asia&quot;, &quot;properties&quot;: {} }, { &quot;name&quot;: &quot;Functions Application Sample&quot;, &quot;type&quot;: &quot;Microsoft.Web/sites&quot;, &quot;kind&quot;: &quot;functionapp&quot;, &quot;apiVersion&quot;: &quot;2016-08-01&quot;, &quot;location&quot;: &quot;Southeast Asia&quot;, &quot;properties&quot;: {} } ], &quot;outputs&quot;: {}}In the ARM template, we can add many things, including every application property, configuration settings‚Ä¶ so it‚Äôs recommended we should update the ARM in every changes in the Azure resource that need to be updated. Since it helps us to keep track what was the change and when the change was effected.The ARM template could be managed in a VS project and automatically deployed in VSTS as well. So I have created one more project in my sample solution like this" }, { "title": "Azure DevOps - What I Have Done", "url": "/posts/azure-devops-what-i-have-done/", "categories": "DevOps", "tags": "azure, vsts", "date": "2018-12-02 00:00:00 +0700", "snippet": "In this post, I will list out all the questions of all the problems I have met when working with the Azure services. Most of them have been answered but some still haven‚Äôt yet. The answers will be in every single next post (or not ü§¶‚Äç‚ôÇÔ∏è)Visual Studio Team Service (VSTS - Azure DevOps) Azure Develop - Build - Release life cycle. How to build only one project in a solution with nuget restore? How to automatically trigger a build for specific branch or specific project? How to prepare a DataFactory project in VS2017? How to build and release (deploy) a DataFactory project on VSTS? How to prepare an ARM template project in VS2017? How to build and release (deploy) an ARM template project on VSTS? How to get the outputs of an ARM template release and set to variable group for cross using in other releases? How to create a resource in another resource group with ARM template? How to get the Azure Functions key and put it to the outputs of ARM template deployment since the function key is only available after the application deployed? How to get the api-version for each azure resource in ARM template? How to get a custom hostname of an app service using ARM template? How to deploy my blog automatically via VSTS after committed to develop branch?Azure Portal How to check the installed certificates in azure app service? How to get the right ARM template of an Azure resource? How to get the content of app.config or web.config in Azure portal?" }, { "title": "How To Use Git With SSH keys", "url": "/posts/how-to-use-git-with-ssh-key/", "categories": "Coding", "tags": "git, ssh", "date": "2018-07-14 00:00:00 +0700", "snippet": "Use CaseIn my use case, I have the existing SSH keys in a .ssh directory like thisI would like to use them to connect to Git repositories such as Github, BitBucket, or VSTS‚Ä¶ using Git tools such as Git for Windows, SourceTree.It should be integrated with CMD Windows application and some built-in Git clients in Visual Studio and VSCode as well.Upload public SSH key to Git repositoriesGithubGo to SSH and GPG keys section in your Github profile page, click to New SSH key button and paste all content in your public SSH key (nhvu1988.pub in my example) to textbox Key and save. The added key will show like this.BitBucketGo to SSH keys section in your Profile Settings page, click to Add key button and paste all content in your public SSH key (nhvu1988.pub in my example) to textbox Key and save. The added key will show like this.Do the same with other Git repositories üòâJust find a SSH keys section in your Profile/Settings page, click to Add key button and paste all content in your public SSH key (nhvu1988.pub in my example) to textbox Key and save.Tools and SetupSourceTree It‚Äôs recommended to use SourceTree rather than Git‚Äôs commands SourceTree‚Äôs a Git GUI client has a very clean, beautiful UI and many powerful features to help you visualize and work on your Git projects. Download SourceTree and install with default settings. Open SourceTree, go to Options and in the General tab, add the path to private SSH key file (*.ppk).It‚Äôs done for SourceTree integration, now you can use SourceTree to connect with Git repositories without username/password.If you only want to use SourceTree to work with Git, you can stop here. The next step is for setting up the integration with other clients such as cmd, VSCode or VS2017.CMD, built-in Git clients in VSCode and VS2017 Download Git for Windows and install with default settings. If you already installed the SourceTree, open Environment Variables and create new User variable with name is GIT_SSH and value is the path to plink.exe existing in SourceTree installed folder. Done! You can connect to any reporitory using CMD, built-in Git clients in VSCode and VS2017.CMD, built-in Git clients in VSCode and VS2017 without SourceTree If you didn‚Äôt install the SourceTree, download pageant.exe and plink.exe. Run pageant.exe and add SSH key file (*.ppk) to pageant key list Open Environment Variables and create new User variable with name is GIT_SSH and value is the path to downloaded plink.exe. Done! You can connect to any reporitory using CMD, built-in Git clients in VSCode and VS2017 without SourceTree.To make Pageant automatically run and load keys at Windows startup Find the location of pageant.exe Press Windows + R to open the ‚Äòrun‚Äô dialog box Type shell:startup in the dialog box Create a shortcut to the pageant.exe and put into this startup folder. Right click on the shortcut and open ‚ÄòProperties‚Äô In Target add: &amp;lt;path to&amp;gt;/pageant.exe myprivatekeyname.ppk In Start in add: &amp;lt;path to *.ppk&amp;gt; Click on the shortcut link and check that Pageant has started and has loaded your keys" }, { "title": "Azure Data Factory - How to deploy on Azure portal", "url": "/posts/data-factory-how-to-deploy-on-azure-portal/", "categories": "DevOps", "tags": "azure, data-factory", "date": "2017-12-03 00:00:00 +0700", "snippet": "In the previous blog, we have prepared 3 basic things that enough for Data Factory can work on. Now I will continue with deploying them to Data Factory through Azure portal.Create Data FactoryAfter logged in to Azure portal, just following steps to create new Data FactoryIn the create popup, fill in the information and click to CreateAnd we have it created.Click to the action Author and deploy` to go to deployment editor with nothing here yet.Deploy Data Factory ResourcesTo create a Linked service, click to New data store and select a type of service. In this case, I will choose Azure SQL DatabaseNow, the editor will show the template for AzureSqlLinkedService, but we have it already, just copy the prepared json in ProdSQLLinkedService.json file to editor and click DeployWhen it‚Äôs deployed successfully, we will see it in Linked services listJust following 3 steps to deploy other resources Select correct type of resource, click to ... More to select dataset or pipeline Copy the prepared json to editor Click to Deploy and check the resource on resources listAfter deployed everything, we will see something like thisData Factory Resource ExplorerClick to Monitor &amp;amp; Manage at Data factory information screenNow we are in the Data Factory‚Äôs resource explorer to verify our task is set up correctly, and check the running activity status as well.To rerun an failed activity or just for debug if you have a change on the resources, use Rerun button on an activity." }, { "title": "Azure Data Factory - How to deploy by powershell", "url": "/posts/data-factory-how-to-deploy-by-powerscript/", "categories": "DevOps", "tags": "azure, data-factory", "date": "2017-12-03 00:00:00 +0700", "snippet": "In this blog, I will introduce how to create an automation script that can deploy the Data Factory‚Äôs resources to Azure with one keypress üÜí. So, I will reuse the resources on Data Factory - 3 basic things post for demonstration.All steps that script will run are Read config.json file to gather configuration information. Login user with Azure account. Setup datafactory. Install Linked services. Install Datasets. Install Pipelines.Setup Config FileSince powershell script can load a json file quickly so we create a config.json file like this{ &quot;location&quot;: &quot;EastUS&quot;, &quot;resourceGroup&quot;: &quot;InDark&quot;, &quot;subscriptionName&quot;: &quot;Developer Program Benefit&quot;, &quot;dataFactoryName&quot;: &quot;indarkdatafactory&quot;, &quot;dataFactoryLinkedServices&quot;: [ &quot;./ProdSQLLinkedService.json&quot;, &quot;./DevSQLLinkedService.json&quot;, ], &quot;dataFactoryDatasets&quot;: [ &quot;./Input_ProdUsersTable.json&quot;, &quot;./Output_DevUsersTable.json&quot;, ], &quot;dataPipelines&quot;: [ &quot;./CopyPipeline.json&quot; ],}The resourceGroup and subscriptionName both are information requires to fill while datafactory is installing.Script to read config file on powershell$config = get-content config.json | ConvertFrom-Json;$location = $config.location;$subscriptionName = $config.subscriptionName;$resourceGroupName = $config.resourceGroup;$dataFactoryName = $config.dataFactoryName;AuthenticationIn the first step, script will ask user to login### Authenticate ###$user = Login-AzureRmAccount;if ($user -eq $null) { Return; }Create Data FactoryScript to setup Data Factory### Create data factory ###$dataFactory = Get-AzureRmDataFactory -ResourceGroupName $resourceGroupName -Name $dataFactoryName -ErrorAction SilentlyContinue;if ($dataFactory -eq $null) { &quot;creating data factory $dataFactoryName...&quot;; $dataFactory = New-AzureRmDataFactory -ResourceGroupName $resourceGroupName -Name $dataFactoryName -Location $location -Force; &quot;data factory $dataFactoryName has been created successfully.&quot;; if ($dataFactory -eq $null) { Write-Error &quot;Cannot create data factory $dataFactoryName&quot;; Stop-Transcript; return; }}else { &quot;data factory $dataFactoryName found&quot;;}Install Linked ServicesScript to install Linked services$linkedServiceFiles = $config.dataFactoryLinkedServices;foreach($fileName in $linkedServiceFiles) { &quot;installing linked services $fileName...&quot;; $linkedService = New-AzureRmDataFactoryLinkedService -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -File $fileName -Force -ErrorAction SilentlyContinue; if ($linkedService -eq $null) { Write-Error &quot;Cannot create linked service $fileName&quot;; Stop-Transcript; return; }}Install DatasetsScript to install Datasets$datasetFiles = $config.dataFactoryDatasets;foreach($fileName in $datasetFiles) { &quot;installing dataset $fileName...&quot;; $dataset = New-AzureRmDataFactoryDataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -File $fileName -Force -ErrorAction SilentlyContinue; if ($dataset -eq $null) { Write-Error &quot;Cannot create dataset $fileName&quot;; Stop-Transcript; return; }}Install PipelinesScript to install Pipelines```powershell$pipelineFiles = $config.dataPipelines;foreach($fileName in $pipelineFiles) { ‚Äúinstalling pipeline $fileName‚Ä¶‚Äù; $pipeline = New-AzureRmDataFactoryPipeline -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -File $fileName -Force -ErrorAction SilentlyContinue; if ($pipeline -eq $null) { Write-Error ‚ÄúCannot create pipeline $fileName‚Äù; Stop-Transcript; return; }}" }, { "title": "Azure Data Factory - Three basic things", "url": "/posts/data-factory-three-basic-things/", "categories": "DevOps", "tags": "azure, data-factory", "date": "2017-12-02 00:00:00 +0700", "snippet": " (Updated) 2020-05-03 - Data Factory V2 was released, see changes here.Azure Data Factory (DF) is an Azure service helps to copy and transform data from data source to another data source, and some use cases that we can use Azure Data Factory Azure are. Restore data from a production database to development database every day. Transfer data from on-premises SQL server database to Azure SQL server database. Backup data from production SQL server database to CSV files every night at 1 AM.There are many use cases that we can use Data Factory, and I will introduce how to work with DF for the first use case that I had been working on it last week.Three basic thingsTo use Data Factory, we have to prepare 3 basic things that Data Factory needs to work. Linked services Datasets PipelinesAll the configuration‚Äôs information wiil be stored in json format.1. Linked servicesLinked services are much like connection strings, which define the information that‚Äôs needed for Data Factory to connect to external resources. For example, the connectionString in below JSON string describes the connection of a SQL server database.{ &quot;name&quot;: &quot;ProdSQLLinkedService&quot;, &quot;properties&quot;: { &quot;type&quot;: &quot;AzureSqlDatabase&quot;, &quot;typeProperties&quot;: { &quot;connectionString&quot;: &quot;Data Source=production.database.windows.net;Initial Catalog=ProdDB;Integrated Security=False;User ID=username;Password=password;&quot; } }}So in this use case, we will have 2 linked services, one is for the production database, and one is for the develop database. ProdSQLLinkedService.json DevSQLLinkedService.json2. DatasetsDatasets represent data structures within the data stores, such as the table structure we want to restore from.{ &quot;name&quot;: &quot;Input_ProdUsersTable&quot;, &quot;properties&quot;: { &quot;type&quot;: &quot;AzureSqlTable&quot;, &quot;linkedServiceName&quot;: &quot;ProdSQLLinkedService&quot;, &quot;structure&quot;: [ { &quot;name&quot;: &quot;UserId&quot;, &quot;type&quot;: &quot;Guid&quot; }, { &quot;name&quot;: &quot;Username&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;DateOfBirth&quot;, &quot;type&quot;: &quot;Datetime&quot; } ], &quot;typeProperties&quot;: { &quot;tableName&quot;: &quot;Users&quot; }, &quot;availability&quot;: { &quot;frequency&quot;: &quot;Day&quot;, &quot;interval&quot;: 1 }, &quot;external&quot;: true, &quot;policy&quot;: {} }}So in this use case, we also will have 2 datasets, one is for the table in production database, and one is for the table in development database. Input_ProdUsersTable.json Output_DevUsersTable.json3. PipelinesA pipeline contains one or more activities that together perform a task. An activity could be a copy data action, a data transform action or a custom action that could do anything we want to process on our data.So in my use case, I will put a copy activity to the pipeline.{ &quot;name&quot;: &quot;CopyPipeline&quot;, &quot;properties&quot;: { &quot;description&quot;: &quot;Copy data in table User from Prod database to Dev database for checking&quot;, &quot;activities&quot;: [ { &quot;name&quot;: &quot;CopyFromSQLToSQL&quot;, &quot;type&quot;: &quot;Copy&quot;, &quot;inputs&quot;: [ { &quot;name&quot;: &quot;Input_ProdUsersTable&quot; } ], &quot;outputs&quot;: [ { &quot;name&quot;: &quot;Output_DevUsersTable&quot; } ], &quot;typeProperties&quot;: { &quot;source&quot;: { &quot;type&quot;: &quot;SqlSource&quot; }, &quot;sink&quot;: { &quot;type&quot;: &quot;SqlSink&quot;, &quot;writeBatchSize&quot;: 10000, &quot;writeBatchTimeout&quot;: &quot;60:00:00&quot; } }, &quot;policy&quot;: { &quot;concurrency&quot;: 1, &quot;retry&quot;: 0, &quot;timeout&quot;: &quot;01:00:00&quot; } } ], &quot;start&quot;: &quot;2016-07-12T00:00:00Z&quot;, &quot;end&quot;: &quot;2099-12-31T00:00:00Z&quot; }}Finally, we will have 2 linked services, 2 datasets and 1 pipeline with a copy activity inside.On the next blog, I will continue with how to deploy them to Data Factory through Azure portal.SummaryThe Azure Data Factory supports many ways to do with the data, but three things above always need to understand at the first step when you walk into this world üòä" }, { "title": "How to use bulk copy program (bcp) utility in SQL Server?", "url": "/posts/how-to-use-bulk-copy-program-bcp-in-sql-server/", "categories": "DevOps", "tags": "sql-server", "date": "2017-09-14 00:00:00 +0700", "snippet": "The bulk copy program utility (bcp) bulk copies data between an instance of Microsoft SQL Server and a data file in a user-specified format‚Ä¶Of course, I don‚Äôt rewrite the whole article about bcp Utility but I just want to record all my understandings about it as very simple examples after one week working on.All commands run on powershell.Generate CSV data fileUsing bcp to generate a CSV file of any SQL data table is very simple like thatbcp &quot;TableName&quot; out &quot;CSVFilePath.csv&quot; -c -T -T argument is using when we connect to SQL Server with a trusted connection using integrated security.Otherwise, -U and -P are required.bcp &quot;TableName&quot; out &quot;CSVFilePath.csv&quot; -c -U &quot;username&quot; -P &quot;password&quot; -S is specify the server name when we connect to a remote SQL server -d is the database name in case the SQL user only has permission on specific databasebcp &quot;TableName&quot; out &quot;CSVFilePath.csv&quot; -c -S &quot;server&quot; -U &quot;username&quot; -P &quot;password&quot; -d &quot;database&quot;We can use -t argument to specific field terminator (here is ; charactor, and the default is \\t -tab- character)bcp &quot;TableName&quot; out &quot;CSVFilePath.csv&quot; -c -T -t &quot;;&quot;If we want to use the query to customize data return, just replace out by queryoutbcp &quot;SELECT * FROM TableName&quot; queryout &quot;CSVFilePath.csv&quot; -c -T -t &quot;;&quot;Generate Native data-type file for exporting and importingBasically the -n or -N argument is using for exporting data from this table and import to another table on another database. It will generate data file with the same data type and native value of data.First, we should generate the format file using format nul and -f argumentbcp &quot;TableName&quot; format nul -f &quot;TableName.fmt&quot; -n -TAnd generate to data filebcp &quot;TableName&quot; out &quot;TableName.bcp&quot; -n -TAnd use this command for importingbcp &quot;ImportTableName&quot; in &quot;TableName.bcp&quot; -f &quot;TableName.fmt&quot; -TOther arguments-w uses to be replaced for -c if table has any field contains Unicode character.-x only use for generating format file, and bcp will generate format file in XML format (default is non-XML format).bcp &quot;TableName&quot; format nul -f &quot;TableName.xml&quot; -n -x -TThis is example of generated .fmt file13.031 SQLINT 0 4 &quot;&quot; 1 CodeType &quot;&quot;2 SQLNCHAR 2 510 &quot;&quot; 2 Description Danish_Norwegian_CI_AI3 SQLCHAR 2 255 &quot;&quot; 3 ValidationPattern Danish_Norwegian_CI_AIAnd this is example of generated .xml file&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;BCPFORMAT xmlns=&quot;http://schemas.microsoft.com/sqlserver/2004/bulkload/format&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&amp;gt; &amp;lt;RECORD&amp;gt; &amp;lt;FIELD ID=&quot;1&quot; xsi:type=&quot;NativeFixed&quot; LENGTH=&quot;4&quot;/&amp;gt; &amp;lt;FIELD ID=&quot;2&quot; xsi:type=&quot;NCharPrefix&quot; PREFIX_LENGTH=&quot;2&quot; MAX_LENGTH=&quot;510&quot; COLLATION=&quot;Danish_Norwegian_CI_AI&quot;/&amp;gt; &amp;lt;FIELD ID=&quot;3&quot; xsi:type=&quot;CharPrefix&quot; PREFIX_LENGTH=&quot;2&quot; MAX_LENGTH=&quot;255&quot; COLLATION=&quot;Danish_Norwegian_CI_AI&quot;/&amp;gt; &amp;lt;/RECORD&amp;gt; &amp;lt;ROW&amp;gt; &amp;lt;COLUMN SOURCE=&quot;1&quot; NAME=&quot;CodeType&quot; xsi:type=&quot;SQLINT&quot;/&amp;gt; &amp;lt;COLUMN SOURCE=&quot;2&quot; NAME=&quot;Description&quot; xsi:type=&quot;SQLNVARCHAR&quot;/&amp;gt; &amp;lt;COLUMN SOURCE=&quot;3&quot; NAME=&quot;ValidationPattern&quot; xsi:type=&quot;SQLVARYCHAR&quot;/&amp;gt; &amp;lt;/ROW&amp;gt;&amp;lt;/BCPFORMAT&amp;gt;" }, { "title": "Ext.NET - What to get it WRONG? - PART 3 - DirectEvent", "url": "/posts/ext-net-what-to-get-it-wrong-part-3-directevent/", "categories": "Coding", "tags": "ext.net", "date": "2017-06-13 07:00:00 +0700", "snippet": "DirectEventsDirectEvent is an event will trigger a DirectEvent ajax request to the server. It could be triggered by grid-column-command event, button-click event or from MessageBusDirectEventDirectEvent often goes with 3 common properties Url, Method, and ExtraParamsDirectEvent ={ Url = builder.UrlHelper.Action(&quot;Action&quot;, &quot;Controller&quot;), Method = HttpMethod.POST, ExtraParams = { new Parameter(&quot;key&quot;, &quot;value&quot;, ParameterMode.Value) }}In Razor styleHtml.X().Button().DirectEvents(de =&amp;gt;{ de.Click.Url = Url.Action(&quot;Action&quot;, &quot;Controller&quot;); de.Click.Method = HttpMethod.POST; de.Click.ExtraParams.Add( Html.X().Parameter() .Name(&quot;key&quot;) .Value(&quot;value&quot;) .Mode(ParameterMode.Value) );})Ext.NET will generate to javascriptExt.net.directRequest({ url: &quot;/Controller/Action&quot;, method: &quot;POST&quot;, extraParams: {&quot;key&quot;:&quot;value&quot;}}DirectEvent PropertiesI will try to list all common properties of DirectEvent that I have worked withUrlUrl = builder.UrlHelper.Action(&quot;Action&quot;, &quot;Controller&quot;)Url is the url of the ajax request, it‚Äôs a string combined from controller name and action method It‚Äôs recommended to use UrlHelper.Action to combine the UrlMethodMethod = HttpMethod.POSTMethod is HTTP method of the ajax request. It could be HttpMethod.POST, HttpMethod.GET, HttpMethod.DELETE‚Ä¶ExtraParamsExtraParams ={ new Parameter(&quot;key&quot;, &quot;value&quot;, ParameterMode.Value)}// orHtml.X().Parameter() .Name(&quot;key&quot;) .Value(&quot;value&quot;) .Mode(ParameterMode.Value)ExtraParams contains all the extra parameters that will be sent in the ajax request. Each extra parameter is a pair of key name and value, and The parameter mode will define value is a hard or dynamic value.If the mode is ParameterMode.Value, Ext.NET will generate the value as hard valueextraParams: {key: &quot;value&quot;} // value in double quote will be a hard-string valueand when the mode is ParameterMode.Raw, the value will be a dynamic valueextraParams: {key: value} // value is a JS variable is available in run-time // otherwise, &quot;value is not defined&quot; exception will throwUse dynamic value in DirectEventThe biggest problem when working with DirectEvent is we often need to use a dynamic extra parameter. It means we use ParameterMode.Raw mode and need to know What variables available in run-time?In the generation to JS part, DirectEvent always comes in a handler function that call Ext.net.directRequest(&#39;...&#39;) in the body.For example, DirectEvent of button clickHtml.X().Button().DirectEvents(de =&amp;gt; { de.Click.ExtraParams.Add( Html.X().Parameter() );});and will be generated with item and e variables as below{ buttons: [{ directEvents: { click: { fn: function(item, e) { Ext.net.directRequest({ extraParams: {} }); } } } }]}So base on every single type of the component DirectEvent is binding, it will be bounded with the difference variables.DirectEvent in ButtonContinue to the previous sample item is the ExtJS button component e is an onclick object event.In this case, we can get name of button in item by using item.name in extra params like thisHtml.X().Parameter() .Name(&quot;buttonName&quot;) // define &quot;buttonName&quot; as key .Value(&quot;item.name&quot;) .Mode(ParameterMode.Raw) // use raw mode to pass dynamic valueand it generates tofn: function(item, e){ Ext.net.directRequest({ extraParams: {&quot;buttonName&quot;: item.name} });}DirectEvent in Grid CommandX.GridPanel() .ColumnModel( X.ImageCommandColumn() .Commands(X.ImageCommand().CommandName(&quot;Submit&quot;)) .DirectEvents(de =&amp;gt; {...}) );It‚Äôs just the same with Button but the difference variablesfn: function (item, command, record, recordIndex, cellIndex) { Ext.net.directRequest({...});} item is the command component, here is the ImageCommand named ‚ÄúSubmit‚Äù. command is name of command calling, ‚ÄúSubmit‚Äù is the command in this case. record is the store record which maps to the referenced cell. We can access to data using record.data. recordIndex is the index of store record which maps to the referenced cell. cellIndex is the index of referenced cell." }, { "title": "Ext.NET - What to get it WRONG? - PART 2 - Layout", "url": "/posts/ext-net-what-get-wrong-part2-layout/", "categories": "Coding", "tags": "ext.net", "date": "2017-06-04 00:00:00 +0700", "snippet": "LayoutThose are the questions I usually ask or got from my co-workers Why this component doesn‚Äôt resize to fit browser‚Äôs size? Why this field doesn‚Äôt fit to its parent? Why GridPanel doesn‚Äôt fill with parent‚Äôs width? In Column Layout, how to fill children with parent‚Äôs height?To answer them, we need to understand how the layout works in Ext.NET first.ViewportViewport oftens be used as a top parent component of the application. It could be re-sized automatically to fill the browser viewport, and be used to control the whole application‚Äôs layout. Viewport is the top most level of layout component" }, { "title": "Ext.NET - What to get it WRONG? - PART 1 - Why we use it?", "url": "/posts/ext-net-what-get-wrong-part1-why-we-use-it/", "categories": "Coding", "tags": "ext.net", "date": "2017-06-03 07:00:00 +0700", "snippet": "Ext.NET - You got it WRONG!Ext.NET is an ASP.NET framework that helps to develop the UI for a web application likes desktop application. It‚Äôs a cool framework with many advantages, comes with a bunch of cool built-in components, and also has a beautiful sample site. But‚Ä¶ Why there are many people in my company scare it off? üò±üò±üò±After a while, I also had a chance to join the team working on it. I realize that the codes have old ages and the implementation contains many mistakes, which could make any newbie misunderstand and be afraid at first time step into the Ext.NET world.In this series, I will try to list out some our mistakes could be improved to make the Ext.NET more friendly and take it easy to work on.Why use Ext.NET?Firstly, we can go through some advantages of Ext.NETBuilt for .NET developersYeah! Ext.NET is built for us, .NET developers üë®‚Äçüíª, from‚Ä¶ its name? ü§∑Haha, it‚Äôs just a joke but it‚Äôs not wrong üôÜ We can use .NET to build the server (backend) and also the client (frontend) so our codes will be consistent in both sides. It‚Äôs a big benefit when we want to stabilize the system, easy to maintain, and quickly figure out what‚Äôs wrong when bug üêû is coming!HTML5 + CSS + JSYeah! It comes with a set of underlying HTML5, CSS and JS components and since it‚Äôs built for .NET developer, we (.NET developers) don‚Äôt require to have a good or deeply knowledge of those web technologies. Of course, basic skills in CSS and JS to do customization is not our problem, right? üòâCross-browser compatibilityYeah! It‚Äôs cross-browser compatibility, supports all modern browsers, includes‚Ä¶ IE6 supported üëçüëçüëçAs you know, IE is a dangerous browser for any front-end developers. In my case, when starting a new project, I always ask my boss what‚Äôs the minimum IE version needs to be supported? üò®ResponsiveYeah! Ext.NET supports responsive. All components supposed to be auto resizable to fit to their defining layout and fit to their parent (the container).It means, when the parent resizes, all the inside components will be resized to fit the parent size and their corresponding chidren also will be resized simultaneously.Ext.NET provides many types of layout to handle most of the popular cases.The border layout is the one I love most üòç. It could be used to layout the whole webpage that have 3 basic things: header, body, and footer, in a viewport, that I didn‚Äôt see any support from other frontend frameworks so far. But we need to understand how the Layout works correctly!Otherwise, fixing layout in Ext.NET could take you a day!Nice document and many cool built-in componentsYeah! The last but not least advantage of Ext.NET is, it comes with a beautiful sample site. On that site, we can see attractive samples of all built-in components and they also provide source codes for us easy to catch up and quickly find out what we need, can use in our application.Why not use Ext.NET?The only reason that you shouldn‚Äôt use Ext.NET is‚Ä¶ You‚Äôre not RICH üòêOh‚Ä¶ Yes! Ext.NET is not free. It requires to buy a license if we want to use it on the commercial application.$4999 is not a cheap price but not expensive compares to its features?Luckily, my company has it, so I don‚Äôt worry about it anymore ü§ë" }, { "title": "How to group on array field with angular-ui-grid", "url": "/posts/how-to-group-on-array-field-with-angular-ui-grid/", "categories": "Coding", "tags": "javascript, angular, ui-grid", "date": "2017-05-02 00:00:00 +0700", "snippet": "The challengeLast week, I got a task to add grouping on angular-ui-grid for the column is an array field. The data isvar movies = [ { title: &#39;Underworld: Blood Wars&#39;, categories: [&#39;Action&#39;, &#39;Horror&#39;] }, { title: &#39;The Bye Bye Man&#39;, categories: [&#39;Horror&#39;] }, { title: &#39;Sleepless&#39;, categories: [&#39;Action&#39;, &#39;Thriller&#39;] }];$scope.gridOptions = { treeRowHeaderAlwaysVisible: false, columnDefs: [ { name: &#39;title&#39;, field: &#39;title&#39; }, { name: &#39;categories&#39;, field: &#39;categories&#39; } ], data: movies}The categories is the field contains a list of categories movie is paired with. By default, ui-grid can‚Äôt handle array field and generates it as string.The expected result is the ui-grid can group the movies by theirs categories one by one. It means category Action contains ‚ÄúUnderworld: Blood Wars‚Äù and ‚ÄúSleepless‚Äù Horror contains ‚ÄúUnderworld: Blood Wars‚Äù and ‚ÄúThe Bye Bye Man‚Äù Thriller only contains ‚ÄúSleepless‚Äù After googling, also asking question on stackoverflow and ui-grid Github repo but I can‚Äôt see any solution available, so I decide to read ui-grid‚Äôs document and try to build a solution.SolutionFlatten array field to text fieldI try to flatten movies to a cloneMovies object has field category as string, that will have duplicate records in title.var cloneMovies = [];// flatten movies to cloneMoviesfor (var i=0; i&amp;lt;movies.length; i++) { var movie = movies[i]; for (var j=0; j&amp;lt;movie.categories.length; j++) { var category = movie.categories[j]; cloneMovies.push({ title: movie.title, category: category }); }}/** * cloneMovies = [ * { title: &#39;Underworld: Blood Wars&#39;, category: &#39;Action&#39; }, * { title: &#39;Underworld: Blood Wars&#39;, category: &#39;Horror&#39; }, * { title: &#39;The Bye Bye Man&#39;, category: &#39;Horror&#39; }, * { title: &#39;Sleepless&#39;, category: &#39;Action&#39; } * { title: &#39;Sleepless&#39;, category: &#39;Thriller&#39; } * ]; */Add hidden field to group flattened text fieldIn this example, I add a hidden category column to gridOptions‚Äôs columnDefs.columnDefs: [ { name: &#39;title&#39;, field: &#39;title&#39; }, { name: &#39;categories&#39;, field: &#39;categories&#39; }, // Adding hidden &quot;category&quot; column { name: &#39;category&#39;, field: &#39;category&#39;, visible: false }],This new column is used for displaying the first grouping column when we do grouping on categories field. In another word, this column only shows when the grid is grouped by categories.Detect grouping event on array field and forward it to text fieldThanks to groupingChanged event of grouping PublicAPI, we can listen whenever the categories is grouped on, by clicking on column header or calling through groupColumn API. I will update the grouping column to category$scope.gridApi.grouping.on.groupingChanged($scope, function(col) { // detect grouping on &#39;categories&#39; column event called // (by clicking on column header or calling through groupColumn API) // we will change the grouping column to &#39;category&#39; if (col.field === &#39;categories&#39; &amp;amp;&amp;amp; col.grouping.groupPriority !== undefined) { $scope.gridApi.grouping.ungroupColumn(&#39;categories&#39;); $scope.gridApi.grouping.groupColumn(&#39;category&#39;); return; }});Show/hide nessesary column and update the correct dataNow, the grid can group on the new category field, but I have to do a little trick to make the switching between 2 columns transparency.$scope.gridApi.grouping.on.groupingChanged($scope, function(col) { // nothing to do unless grouping field is &#39;category&#39; if (col.field !== &#39;category&#39;) return; if (col.grouping.groupPriority !== undefined) { // grouping by category is turning on $scope.gridOptions.columnDefs[1].visible = false; // hide column categories $scope.gridOptions.columnDefs[2].visible = true; // show column category $scope.gridOptions.data = cloneMovies; // update data to use clone object } else { // grouping by category is turning off $scope.gridOptions.columnDefs[1].visible = true; // show column categories $scope.gridOptions.columnDefs[2].visible = false; // hide column category $scope.gridOptions.data = movies; // update data to use original object }});Finally, it works like a charm :D. Please check out my demo on jsfiddle to see how it works." }, { "title": "How I write a chatbot?", "url": "/posts/how-i-write-a-bot/", "categories": "Coding", "tags": "nodejs, bot framework, luis", "date": "2017-02-06 00:00:00 +0700", "snippet": "It‚Äôs just an internal chatbot for my company, it‚Äôs available on skype/messenger/slack and will answer some questions like How many hours did I work last year? Get Vu‚Äôs working hours yesterday? Get Vu‚Äôs working hours on 1st November 2016? Get my hours from Jan - Mar 2016 Get my hours in week 23, 2016Some technologies I learned and used for this project are Bot Framework to build and deploy chatbot LUIS to understand human language Chrono a natural language date parser in JavascriptBot Framework and LUISAfter some researching, I decide to use Bot Framework to build my chatbot since it supports nodejs which‚Äôs my favorite programming language.I use LUIS to add natural language understanding to my bot. I compared it with api.ai and wit.ai and see it has a good design, friendly UI to define an intent, add new utterance, train and publish the application. They also have 2 videos for me can easily get the overview of LUIS üòç.LUIS - builtin.datetimeSeem everything relates to coding with bot framework, training LUIS to understand intent is easy‚Ä¶ until I have many types of Date Ranges need to be supported in my chatbot. For example, in 5 questions above, I need to parse date range texts such as today, yesterday, last year, Jan - Mar 2016, Week 23, 2016‚Ä¶First of all, thanks to LUIS, it has a set of pre-built entities to recognize date and time in builtin.datetime. It will understand when I say yesterday, 1st November or last week, in 3 first questions above, and will convert that to a date.// tomorrow&quot;resolution&quot;: { &quot;date&quot;: &quot;2016-11-20&quot; }// last quarter&quot;resolution&quot;: { &quot;date&quot;: &quot;XXXX-Q4&quot; }// last year&quot;resolution&quot;: { &quot;date&quot;: &quot;2015&quot; }// last two years&quot;resolution&quot;: { &quot;duration&quot;: &quot;P2Y&quot; }// last week&quot;resolution&quot;: { &quot;date&quot;: &quot;2016-W45&quot; }// past three weeks&quot;resolution&quot;: { &quot;duration&quot;: &quot;P3W&quot; }// this month&quot;resolution&quot;: { &quot;date&quot;: &quot;2016-11&quot; }// last ten months&quot;resolution&quot;: { &quot;duration&quot;: &quot;P10M&quot; }ChronoSince the LUIS with builtin datetime did good jobs on parsing Datetime, the next requirement is how I can parse a date ranges from a specific date (or month) to another specific date (or month)? For example in my fourth question, it‚Äôs Jan - Mar 2016Chrono supports most date and time formats, such as: Today, Tomorrow, Yesterday, Last Friday, etc 17 August 2016 - 19 August 2016 January - March 2016 5 days agoSo I used it to try to recognize if LUIS couldn‚Äôt give me any information about date ranges. Two things I would like to note is I had to use every values in the knownValues to parse a date and always parse date in UTCsince the date object on server can be in difference timezonevar chrono = require(&#39;chrono-node&#39;);var chronoParsedResults = chrono.parse(message);if (chronoParsedResults &amp;amp;&amp;amp; chronoParsedResults.length) { // we just want to get the first parsed value var firstResult = chronoParsedResults[0]; console.log(&#39;firstResult start&#39;, firstResult.start); // we always can get start date from chrono var startValues = firstResult.start.knownValues; var startDate = createUTCDateFromKnownValues(startValues.year, startValues.month, startValues.day, true); //...}function createUTCDateFromKnownValues(year, month, day, isFrom) { // (year, null, null, ?) if (year &amp;amp;&amp;amp; !month &amp;amp;&amp;amp; !day) return isFrom ? new Date(Date.UTC(year, 0, 1)) : new Date(Date.UTC(year, 11, 31, 23, 59, 59)); // (year, month, null, ?) if (year &amp;amp;&amp;amp; month &amp;amp;&amp;amp; !day) { if (isFrom) return new Date(Date.UTC(year, month - 1, 1)); var firstNextMonth = new Date(Date.UTC(year, month, 1, 23, 59, 59)); firstNextMonth.setDate(firstNextMonth.getDate() - 1); return firstNextMonth; } // (year, month, day, ?) if (year &amp;amp;&amp;amp; month &amp;amp;&amp;amp; day) { return isFrom ? new Date(Date.UTC(year, month - 1, day)) : new Date(Date.UTC(year, month - 1, day, 23, 59, 59)); }}Parsing Week Of YearThe last challenge is I would like to parse date ranges for a specific week in year, for example is week 23, 2016, week 1 (default year is current year). But LUIS or chrono doesn‚Äôt support it yet (hopefully it will support in future üòÇ). So I had to define an entity WeekOfYear to recognizeand try to parse with momentvar weekOfYearEntity = builder.EntityRecognizer.findEntity(entities, &#39;WeekOfYear&#39;);var weekOfYear = weekOfYearEntity ? weekOfYearEntity.entity : &#39;&#39;;// we check if week of year found on messageif (weekOfYear) { var definedWeekOfYearFormat = [&#39;[week] W YYYY&#39;, &#39;[week] W, YYYY&#39;]; var m = moment(weekOfYear, definedWeekOfYearFormat); //...}" }, { "title": "Validate date string in javascript", "url": "/posts/validate-date-string-javascript/", "categories": "Coding", "tags": "javascript, moment", "date": "2016-11-21 00:00:00 +0700", "snippet": "Use caseToday I got a question from my colleague for validating a date string in javascript.After quick search on Google, I still don‚Äôt see have any solution for this so I write here to take note for reuse later :)We can use moment library and standard format ISO_8601 to checkmoment(date_str, moment.ISO_8601).isValid();Examplesmoment(&#39;2013-02-04T10:35:24-08:00&#39;, moment.ISO_8601).isValid(); // -&amp;gt; truemoment(&#39;2013-02-04&#39;, moment.ISO_8601).isValid(); // -&amp;gt; truemoment(&#39;2013-02-04 10:35&#39;, moment.ISO_8601).isValid(); // -&amp;gt; truemoment(&#39;2013-02-30 10:35&#39;, moment.ISO_8601).isValid(); // -&amp;gt; false, no 30 in Feb :)moment(&#39;x&#39;, moment.ISO_8601).isValid(); // -&amp;gt; falsemoment(&#39;x 1&#39;, moment.ISO_8601).isValid(); // -&amp;gt; false" }, { "title": "Prepositions of Time - at, in, on", "url": "/posts/on-in-at-time/", "categories": "English", "tags": "prepositions, time", "date": "2016-10-06 00:00:00 +0700", "snippet": "Rules at for a PRECISE TIME in for MONTHS, YEARS, CENTURIES and LONG PERIODS on for DAYS and DATES PRECISE TIME MONTHS, YEARS, CENTURIES and LONG PERIODS DAYS and DATES at 3 o‚Äôclock in May on Sunday at 10.30am in summer on Tuesdays at noon in the summe on 6 March at dinnertime in 1990 on 25 Dec. 2010 at bedtime in the 1990s on Christmas Day at sunrise in the next century on Independence Day at sunset in the Ice Age on my birthday at the moment in the past/future on New Year‚Äôs Eve Expressions at in on at night in the morning on Tuesday morning at the weekend* in the mornings on Saturday mornings at Christmas*/Easter in the afternoon(s) on Sunday afternoon(s) at the same time in the evening(s) on Monday evening(s) at present ¬† ¬† Note that in some varieties of English people say ‚Äúon the weekend‚Äù and ‚Äúon Christmas‚Äù.Examples I have a meeting at 9am. The shop closes at midnight. Jane went home at lunchtime. In England, it often snows in December. Do you think we will go to Jupiter in the future? reference: englishclub" }, { "title": "How to edit text in vim editor?", "url": "/posts/how-to-edit-text-in-vim/", "categories": "DevOps", "tags": "vim", "date": "2016-09-20 00:00:00 +0700", "snippet": " Press a to start editable state Edit text Press esc to stop editable state Type :wq (write &amp;amp; quit) to save and stop editor" }, { "title": "&quot;On a Newspaper&quot; or &quot;In a Newspaper&quot;?", "url": "/posts/on-or-in-a-newspaper/", "categories": "English", "tags": "preposition", "date": "2016-08-12 00:00:00 +0700", "snippet": "Rules Use IN with printed materials: in a book, in a newspaper Use ON with electronics: on TV, on the radio, on the InternetMore examples I read that news in the newspaper I always hear news on TV or read news on Facebook You can check that word in the dictionary on page 123" }, { "title": "How to merge multi commits to one commit", "url": "/posts/merge-multi-commits-to-one-commit/", "categories": "DevOps", "tags": "git-merge", "date": "2016-08-09 00:00:00 +0700", "snippet": "Merge all diff commits in another branchFirstly, we should start working new feature/bugfix on a new branch and commit as much as we want.After finished coding, we can merge them to master branch with one combination commit using merge --squash command# switch to master branchgit merge --squash &amp;lt;feature branch&amp;gt;" }, { "title": "How to get saved wifi password in windows?", "url": "/posts/how-to-get-saved-wifi-password-in-windows/", "categories": "DevOps", "tags": "wifi, password", "date": "2016-08-05 00:00:00 +0700", "snippet": " Open CMD in Administration Run below command to see all saved wifi profiles in your PC. Pick the wifi name to see password netsh wlan show profile Run below command (replaces hoangvu to your wifi name) and see the saved password in Key Content section netsh wlan show profile hoangvu key=clear " } ]
